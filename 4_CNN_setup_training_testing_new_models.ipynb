{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN_training_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f0nIhYPcOgi"
      },
      "source": [
        "# Set up the CNN + model, Train it and Test it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwHi5FI7cOhG"
      },
      "source": [
        "**Before running this script, make sure that your Google Drive folder contains the tiles you created, the `tiling_scheme.json` file (`step 1`), and the 5 `csv` files that you created (`step 3`) to describe: the 3 data subsets (1 `csv` file), the annotations for each (3 `csv` files) and the class list (1 `csv` file).**\n",
        "\n",
        "This code section is adapted from the following guide:\n",
        "https://medium.com/@tabdulwahabamin/an-introduction-to-implementing-retinanet-in-keras-for-multi-object-detection-on-custom-dataset-be746024c653"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/CNN_tools/blob/main/4_CNN_setup_training_testing_new_models.ipynb#scrollTo=view-in-github\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "#####  <center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XElcTShEQkh"
      },
      "source": [
        "### Connect to our Google Drive folder and pull all data\n",
        "Note: when you run this it will give you a link that you must click. You must give Google some permissions, then copy a code into a box that comes up in the output section of this code.\n",
        "\n",
        "If customizing this code, you will need to point the `drive_folder` variable to a URL for your shared google drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le3bfsAjzSP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc6f17a-d252-4cb4-90f4-26ed9338a751"
      },
      "source": [
        "# set variable to the destination google drive folder you want to pull from\n",
        "drive_folder = 'https://drive.google.com/drive/folders/1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D'\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "\n",
        "# this bit points the code to that google drive folder\n",
        "pointer = str(\"'\" + drive_folder.split(\"/\")[-1] + \"'\" + \" in parents\")\n",
        "\n",
        "file_list = drive.ListFile(\n",
        "    {'q': pointer}).GetList()\n",
        "\n",
        "#    this bit pulls every file in the directory specified above\n",
        "image_list = []\n",
        "count = 0\n",
        "countb = 0\n",
        "for f in file_list:\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  countb += 1\n",
        "  if fname.endswith(\".png\"):\n",
        "    image_list.append(fname.split(\"/\")[1])\n",
        "    count += 1\n",
        "    if count % 10 == 0:\n",
        "      print(\"{c} out of {l} tiles pulled\".format(c = str(count), l=str(len(file_list)-(countb - count))))\n",
        "  # 3. Create & download by id.\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n",
        "  if fname.endswith(\".csv\") or fname.endswith(\".json\"):\n",
        "    print(\"Pulled file: \" + fname)\n",
        "print(\"Complete: \" + str(count) + \" tiles pulled\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulled file: data/tiling_scheme.json\n",
            "Pulled file: data/new_detections.json\n",
            "Pulled file: data/classes.csv\n",
            "Pulled file: data/annotations_train.csv\n",
            "Pulled file: data/annotations_test.csv\n",
            "Pulled file: data/subset_list.csv\n",
            "Pulled file: data/annotations_valid.csv\n",
            "Pulled file: data/via_SealCNN_TrainingData.csv\n",
            "10 out of 244 tiles pulled\n",
            "20 out of 244 tiles pulled\n",
            "30 out of 244 tiles pulled\n",
            "40 out of 244 tiles pulled\n",
            "50 out of 244 tiles pulled\n",
            "60 out of 244 tiles pulled\n",
            "70 out of 244 tiles pulled\n",
            "80 out of 244 tiles pulled\n",
            "90 out of 244 tiles pulled\n",
            "100 out of 244 tiles pulled\n",
            "110 out of 244 tiles pulled\n",
            "120 out of 244 tiles pulled\n",
            "130 out of 244 tiles pulled\n",
            "140 out of 244 tiles pulled\n",
            "150 out of 244 tiles pulled\n",
            "160 out of 244 tiles pulled\n",
            "170 out of 244 tiles pulled\n",
            "180 out of 244 tiles pulled\n",
            "190 out of 244 tiles pulled\n",
            "200 out of 244 tiles pulled\n",
            "210 out of 244 tiles pulled\n",
            "220 out of 244 tiles pulled\n",
            "230 out of 244 tiles pulled\n",
            "240 out of 244 tiles pulled\n",
            "Complete: 244 tiles pulled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVqZgStvmrrI"
      },
      "source": [
        "### Identify necessary files from the input directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAvrvhCUmsZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21686628-fd0b-4444-e68e-cd4f415f4b76"
      },
      "source": [
        "import csv, json, glob\n",
        "\n",
        "# use this variable to set input directory\n",
        "input_dir = local_download_path\n",
        "\n",
        "training_data_file = 'annotations_train.csv'\n",
        "testing_data_file = 'annotations_test.csv'\n",
        "validation_data_file = 'annotations_valid.csv'\n",
        "classes_file = 'classes.csv'\n",
        "subset_list_file = 'subset_list.csv'\n",
        "tiling_scheme_file = 'tiling_scheme_placeholder'\n",
        "\n",
        "checklist = {training_data_file:\"training_data_file\", testing_data_file:\"testing_data_file\", \n",
        "             validation_data_file:\"validation_data_file\", classes_file:\"classes_file\",\n",
        "             subset_list_file:\"subset_list_file\", tiling_scheme_file:\"tiling_scheme_file\"}\n",
        "\n",
        "for fname in os.listdir(input_dir):\n",
        "  if fname.endswith(\".csv\"): \n",
        "    try: \n",
        "      vars()[checklist[fname]] = \"{i}/{f}\".format(i=input_dir, f=fname)\n",
        "      print(\"required file found: {v}\".format(v=vars()[checklist[fname]]))\n",
        "      del checklist[fname]\n",
        "    except: print(\"{f} detected but not listed among requirements\".format(f=fname))\n",
        "  if fname.endswith(\".json\"):\n",
        "    tiling_scheme_candidate = \"{i}/{f}\".format(i=input_dir, f=fname)\n",
        "    with open(tiling_scheme_candidate) as f:\n",
        "      try:\n",
        "        tile_list = json.load(f)[\"tile_pointers\"][\"image_locations\"]\n",
        "        tiling_scheme_file = tiling_scheme_candidate\n",
        "        print(\"required file found: {s}\".format(s=tiling_scheme_file))\n",
        "        del checklist['tiling_scheme_placeholder']\n",
        "      except: \n",
        "        print(\"{f} detected but not listed among requirements\".format(f=fname))\n",
        "\n",
        "if len(checklist) > 0:\n",
        "  for key in checklist:\n",
        "    print(\"Error: did not find {k} in your input folder\".format(k=key))\n",
        "  raise Exception(\"missing specified data files\")\n",
        "  \n",
        "# confirm that all files in tiling_scheme_file were pulled from Google Drive\n",
        "if len(tile_list) != len(image_list):\n",
        "  print('Step one produced {n1} tiles, but google drive contained {n2} images. Confirm that tile set is complete.\\n'.format(n1=len(tile_list), n2=len(image_list)))\n",
        "  raise Exception(\"tile count mismatch\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_detections.json detected but not listed among requirements\n",
            "required file found: data/tiling_scheme.json\n",
            "required file found: data/annotations_valid.csv\n",
            "via_SealCNN_TrainingData.csv detected but not listed among requirements\n",
            "required file found: data/classes.csv\n",
            "required file found: data/annotations_test.csv\n",
            "required file found: data/annotations_train.csv\n",
            "required file found: data/subset_list.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-xdfna11L2F"
      },
      "source": [
        "### Install the Convolutional Neural Network that will do the detections. \n",
        "\n",
        "This section sets up the software and pulls code for a CNN model called \"RetinaNet\" which uses the model \"ResNet-50\" as a subcomponent. This section then loads data for an existing ResNet-50 model (pre-trained for object detection) which we will further train for our task.\n",
        "\n",
        "Disregard any errors or prompts to \"restart runtime\" unless the code stops progressing (then email me at gdl10@duke.edu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-VXXOfd-LXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbcb07c6-69f8-45a3-f781-bc7a037e6086"
      },
      "source": [
        "# clear colab's current versions\n",
        "!pip uninstall -y keras\n",
        "!pip uninstall -y keras-nightly\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install h5py==2.10.0  \n",
        "\n",
        "# install the keras package we need\n",
        "!pip3 install keras==2.4.3\n",
        "# install the TF version we need\n",
        "!pip3 uninstall tensorflow -y\n",
        "!pip3 install tensorflow==2.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.7.0\n",
            "Uninstalling keras-2.7.0:\n",
            "  Successfully uninstalled keras-2.7.0\n",
            "\u001b[33mWARNING: Skipping keras-nightly as it is not installed.\u001b[0m\n",
            "Found existing installation: tensorflow 2.7.0\n",
            "Uninstalling tensorflow-2.7.0:\n",
            "  Successfully uninstalled tensorflow-2.7.0\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-vis 0.4.1 requires keras, which is not installed.\u001b[0m\n",
            "Successfully installed h5py-2.10.0\n",
            "Collecting keras==2.4.3\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4.3) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.4.3\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Collecting tensorflow==2.3.0\n",
            "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 53 kB/s \n",
            "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 84.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
            "Installing collected packages: numpy, tensorflow-estimator, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 numpy-1.18.5 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jb7YRXlEUUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63aeac1f-3aa5-4ab1-f0b9-0a594852a789"
      },
      "source": [
        "# copy the files for RetinaNet\n",
        "# note that this build is now deprecated, but we are fine with that\n",
        "# now pulling from a personal clone that outputs error metrics\n",
        "! git clone https://github.com/gl7176/keras-retinanet.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'keras-retinanet'...\n",
            "remote: Enumerating objects: 6236, done.\u001b[K\n",
            "remote: Total 6236 (delta 0), reused 0 (delta 0), pack-reused 6236\u001b[K\n",
            "Receiving objects: 100% (6236/6236), 13.48 MiB | 18.40 MiB/s, done.\n",
            "Resolving deltas: 100% (4221/4221), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuULO44w6yx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc103b18-2a34-4c3f-b327-034b7b09f2d9"
      },
      "source": [
        "# change directory and install RetinaNet from the copied code\n",
        "% cd keras-retinanet\n",
        "\n",
        "! pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/keras-retinanet\n",
            "Processing /content/keras-retinanet\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting keras-resnet==0.2.0\n",
            "  Downloading keras-resnet-0.2.0.tar.gz (9.3 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (0.29.24)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (4.1.2.30)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (3.38.0)\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.13)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->keras-retinanet==1.0.0) (2.5.6)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-1.0.0-cp37-cp37m-linux_x86_64.whl size=169810 sha256=1f2c739185bdf23a34bf970569509e186d25ad9cef90c06209c82d29ae5d65db\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/29/34/9b33c07f08b1be9e77607c1fc6b08c679489aa7ddaed329652\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20486 sha256=e307d9e0f7c139728d4002f588af841b41acf5cbecf337767b90deccf1d6ac43\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/ef/06/5d65f696360436c3a423020c4b7fd8c558c09ef264a0e6c575\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "Successfully installed keras-resnet-0.2.0 keras-retinanet-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_IOPbC44SC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d23d18e-bdb0-40e5-ba8b-8cb6be92bad4"
      },
      "source": [
        "! python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build_ext\n",
            "cythoning keras_retinanet/utils/compute_overlap.pyx to keras_retinanet/utils/compute_overlap.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/keras-retinanet/keras_retinanet/utils/compute_overlap.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'keras_retinanet.utils.compute_overlap' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/keras_retinanet\n",
            "creating build/temp.linux-x86_64-3.7/keras_retinanet/utils\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -c keras_retinanet/utils/compute_overlap.c -o build/temp.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kkeras_retinanet/utils/compute_overlap.c:620\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/keras_retinanet\n",
            "creating build/lib.linux-x86_64-3.7/keras_retinanet/utils\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.o -o build/lib.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.cpython-37m-x86_64-linux-gnu.so -> keras_retinanet/utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33S3M0SoI1JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f9c1ec-fe07-4b5d-a6b1-e0146a0f7c11"
      },
      "source": [
        "# if you run this multiple times in a session it will break your pathing\n",
        "% cd ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "resnet_model = \"resnet152\"\n",
        "\n",
        "if resnet_model == \"resnet50\":\n",
        "  resnet_model_name = \"retinanet_resnet50_500_classes_0.4594.h5\"\n",
        "elif resnet_model == \"resnet101\":\n",
        "  resnet_model_name = \"retinanet_resnet101_500_classes_0.4986.h5\"\n",
        "elif resnet_model == \"resnet152\":\n",
        "  resnet_model_name = \"retinanet_resnet152_500_classes_0.4991.h5\"\n",
        "\n",
        "#! wget -P data \"https://github.com/ZFTurbo/Keras-RetinaNet-for-Open-Images-Challenge-2018/releases/download/v1.3/retinanet_resnet152_500_classes_0.4991.h5\"\n",
        "\n",
        "subprocess.run(['wget', '-P', 'data', 'https://github.com/ZFTurbo/Keras-RetinaNet-for-Open-Images-Challenge-2018/releases/download/v1.3/{r}'.format(r=resnet_model_name)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGs_ERCpePXf",
        "outputId": "e58979cd-0750-4896-f4e1-daeb68d9b35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['wget', '-P', 'data', 'https://github.com/ZFTurbo/Keras-RetinaNet-for-Open-Images-Challenge-2018/releases/download/v1.3/retinanet_resnet152_500_classes_0.4991.h5'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYBUnE6wEKuq"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzdoAR2GEKuv"
      },
      "source": [
        "We're giving our model the pre-trained weights that we downloaded above, and then we're telling it to use the `training_data_file`, to run with hyper-parameters `epoch_number` and `batch_size_number`.\n",
        "\n",
        "An epoch is a group of steps after which the model calculates its accuracy; the epoch parameter is the maximum number the training will run before stopping; in this framework the model will stop running once it stops improving (based on mAP) for multiple epochs, determined by a 'patience' variable (here, 5).\n",
        "\n",
        "A step is an increment of training the model on one batch or subset of files. The step size is limited on the upper bound by the training data (divided into batches), which we calculate in the code.\n",
        "\n",
        "A batch is the number of images being analyzed in each step. Batch-size is functionally limited by RAM (how many images the computer can store in memory), and given our default tile size, Colab runs out of memory at batch sizes larger than 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94Xz9e-HWdIa",
        "outputId": "b67085e3-101d-416d-e220-cdb13733cfbb"
      },
      "source": [
        "# Pull the total number of training images so we can calculate the maximum step number\n",
        "\n",
        "import csv\n",
        "\n",
        "training_subset_count = 0\n",
        "with open(subset_list_file) as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      if \"training\" in row:\n",
        "        training_subset_count += 1\n",
        "print(training_subset_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q_jz-8wdJ-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d257d06e-8a46-4964-8691-03c22507602c"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "epoch_number = 50\n",
        "batch_size_number = 2\n",
        "step_number = int(training_subset_count/batch_size_number)\n",
        "print(str(step_number) + \" steps possible\")\n",
        "\n",
        "# the following code gets passed to the terminal, but we've moved it into a\n",
        "# subprocess that pulls variables instead; use terminal code for troubleshooting\n",
        "# or for teaching when you want to see the training in action and use custom variables\n",
        "\n",
        "# (uncommented in for teaching purposes)\n",
        "#! keras-retinanet/keras_retinanet/bin/train.py \\\n",
        "#--weights data/resnet50_coco_best_v2.1.0.h5 \\\n",
        "#--epochs 20 --steps 10 --batch-size 2 \\\n",
        "#csv data/annotations_train.csv data/classes.csv \\\n",
        "#--val-annotations data/annotations_valid.csv\n",
        "\n",
        "# this process takes a while to run, be warned! Consider running in terminal commands\n",
        "# (commented out above) if you want to see the live output as it's running\n",
        "\n",
        "# you can also monitor epoch outputs by output files in the \"output\" folder\n",
        "\n",
        "# (commented out for teaching purposes)\n",
        "#model_run = subprocess.check_output(['keras-retinanet/keras_retinanet/bin/train.py', str('--backbone=\"{r}\"'.format(r=resnet_model)),\n",
        "model_run = subprocess.check_output(['keras-retinanet/keras_retinanet/bin/train.py', str('--backbone={r}'.format(r=resnet_model)),\n",
        "                 '--weights', str('data/' + resnet_model_name),\n",
        "                 '--epochs', str(epoch_number),  '--steps', str(step_number), '--batch-size',\n",
        "                 str(batch_size_number), 'csv', training_data_file, classes_file,\n",
        "                 '--val-annotations', validation_data_file]).decode(\"utf-8\")\n",
        "print(model_run)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105 steps possible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28P8eyNJ2BnO"
      },
      "source": [
        "list_of_files = glob.glob('snapshots/resnet*.h5')\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "epoch_final = latest_file[latest_file.index(\"_csv_\")+5:-3]\n",
        "best_model_training = latest_file.replace(\"/content/\", \"\")\n",
        "print(best_model_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apHCQjSo8WC2"
      },
      "source": [
        "This next section converts the model from training mode to inference mode so it can be used to detect our target objects (seals). Until now we've been updating the model based on its performance; now we're fixing the model in a static \"snapshot\" so we can test it out. This conversion process take a little time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YyMralKEKuz"
      },
      "source": [
        "# note that we are naming our model \"best_model_inference\" and locating it in the \"snapshots\" directory. Customize if wanted\n",
        "model_name = resnet-model + \"best_model_inference\"\n",
        "#! keras-retinanet/keras_retinanet/bin/convert_model.py snapshots/resnet50_csv_10.h5 snapshots/best_model_inference.h5\n",
        "subprocess.run([\"keras-retinanet/keras_retinanet/bin/convert_model.py\", best_model_training, \"snapshots/{m}.h5\".format(m=model_name)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3yk48fEKu2"
      },
      "source": [
        "### Run Detection in inference mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbTLLmbGEKu2"
      },
      "source": [
        "This section sets up the environment, importing modules for python tasks and specific to keras+retinanet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj-WmUXiEKu3"
      },
      "source": [
        "# show images inline\n",
        "%matplotlib inline\n",
        "\n",
        "# automatically reload modules when they have changed\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# import keras\n",
        "import keras\n",
        "\n",
        "# import keras_retinanet\n",
        "from keras_retinanet import models\n",
        "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
        "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
        "from keras_retinanet.utils.colors import label_color\n",
        "\n",
        "# import miscellaneous modules\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "from random import shuffle\n",
        "\n",
        "# set tf backend to allow memory to grow, instead of claiming everything\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_session():\n",
        "    config = tf.compat.v1.ConfigProto()    \n",
        "    config.gpu_options.allow_growth = True\n",
        "    return tf.compat.v1.Session(config=config)\n",
        "\n",
        "# use this environment flag to change which GPU to use\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "# set the modified tf session as backend in keras\n",
        "tf.compat.v1.keras.backend.set_session(get_session())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w987GUwIEKu5"
      },
      "source": [
        "### Load RetinaNet model\n",
        "\n",
        "Now we will load the model that you just converted into inference mode: by default it is called `best_model_inference.h5`, but you might have renamed your model_name variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDo883mYEKu6"
      },
      "source": [
        "model_path = 'snapshots/{m}.h5'.format(m=model_name)\n",
        "\n",
        "print(model_path)\n",
        "\n",
        "# load retinanet model\n",
        "model = models.load_model(model_path, backbone_name=resnet_model)\n",
        "\n",
        "# you can check out a model summary by uncommenting this line\n",
        "#print(model.summary())\n",
        "\n",
        "# load label to names mapping for visualization purposes\n",
        "# pull labels from classes.csv\n",
        "import csv\n",
        "with open(classes_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\",\")\n",
        "    labels_to_names = {int(i[1]):i[0] for i in reader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jfMnDLVEKu9"
      },
      "source": [
        "### Load test imagery\n",
        "\n",
        "Now we will load the \"testing\" subset of images that we downloaded into our data directory during setup (as listed in the subsets CSV file). Just to check, we'll print out the first five names of those images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QjtUN5wEKu9"
      },
      "source": [
        "# load imagery\n",
        "image_dir = \"data/\"\n",
        "\n",
        "# this code pulls only files from the test or validation subset\n",
        "# as specified in this variable, \"target_subset\" either \"test\" or \"validation\"\n",
        "target_subset = \"testing\"\n",
        "\n",
        "image_list = []\n",
        "with open(subset_list_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\",\")\n",
        "    for i in reader:\n",
        "      if i[1] == target_subset:\n",
        "        image_list.append(image_dir + i[0])\n",
        "print(image_list[:5])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detiYRZWEKvA"
      },
      "source": [
        "### Test out detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDezuFxm-xAf"
      },
      "source": [
        "Now we'll visualize some detections from our model to see how it performs. Each detection has a \"confidence score\" that describes the CNN's confidence that the detection is correct. Change the minimum confidence score (the first line of code) and re-run the code to check out how your \"confidence threshold\" affects the numbers of false positives and false negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I13IyhvEKvA"
      },
      "source": [
        "min_score = 0.4 # this is the CNN's confidence that the detection is correct\n",
        "detection_iterations = 10 # max number of images to visualize\n",
        "\n",
        "visualize = True\n",
        "\n",
        "detections = {}\n",
        "\n",
        "total_time = 0\n",
        "\n",
        "count = 0\n",
        "shuffle(image_list)\n",
        "\n",
        "for image_path in image_list:\n",
        "    if count > detection_iterations:\n",
        "        break\n",
        "    else:\n",
        "        count +=1\n",
        "        \n",
        "    image = read_image_bgr(image_path)\n",
        "\n",
        "    if visualize:\n",
        "        # copy to draw on\n",
        "        draw = image.copy()\n",
        "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # preprocess image for network\n",
        "    image = preprocess_image(image)\n",
        "    image, scale = resize_image(image)\n",
        "\n",
        "    # process image\n",
        "    start = time.time()\n",
        "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
        "    total_time += time.time() - start\n",
        "\n",
        "    # correct for image scale\n",
        "    boxes /= scale\n",
        "    if any(score >= min_score for score in scores[0]):\n",
        "        detections[image_path] = []\n",
        "\n",
        "    # visualize detections\n",
        "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
        "        # scores are sorted so we can break\n",
        "        if score < min_score:\n",
        "            break\n",
        "\n",
        "        #print(score)\n",
        "        #print(box)\n",
        "\n",
        "        # TODO this does create a slight error in the boxes, might be worth doing something like\n",
        "        # list(map(str, box) but then would need to cast on the other end back to float\n",
        "        b = box.astype(int)\n",
        "        detections[image_path].append({\"box\" : b, \"label\" : label, \"score\" : score})\n",
        "\n",
        "        if visualize:\n",
        "            color = label_color(label)\n",
        "\n",
        "            # b = box.astype(int)\n",
        "            draw_box(draw, b, color=color)\n",
        "\n",
        "            caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
        "            draw_caption(draw, b, caption)\n",
        "\n",
        "    if any(score >= min_score for score in scores[0]):\n",
        "        if visualize:\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.axis('off')\n",
        "            plt.imshow(draw)\n",
        "            plt.show()\n",
        "    \n",
        "print(\"Finished, time per image:\", total_time/len(image_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Khk5BWEKvD"
      },
      "source": [
        "### Run Detections on all tiles\n",
        "This section repeats the process we just tested for all tiles that make up our orthomosaic. If you want to experiment, you can vary the confidence threshold and the amount of time the model trains, then look at how it affects the resulting detections.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Tvuim_vEKvE"
      },
      "source": [
        "visualize = False\n",
        "min_score = min_score # this is the CNN's confidence that the detection is correct\n",
        "\n",
        "detections = {}\n",
        "\n",
        "total_time = 0\n",
        "\n",
        "for image_path in image_list:\n",
        "       \n",
        "    image = read_image_bgr(image_path)\n",
        "\n",
        "    if visualize:\n",
        "        # copy to draw on\n",
        "        draw = image.copy()\n",
        "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # preprocess image for network\n",
        "    image = preprocess_image(image)\n",
        "    image, scale = resize_image(image)\n",
        "\n",
        "    # process image\n",
        "    start = time.time()\n",
        "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
        "    total_time += time.time() - start\n",
        "\n",
        "    # correct for image scale\n",
        "    boxes /= scale\n",
        "    if any(score >= min_score for score in scores[0]):\n",
        "        detections[image_path] = []\n",
        "\n",
        "    # visualize detections\n",
        "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
        "        # scores are sorted so we can break\n",
        "        if score < min_score:\n",
        "            break\n",
        "\n",
        "        #print(score)\n",
        "        #print(box)\n",
        "\n",
        "        # TODO this does create a slight error in the boxes, might be worth doing something like\n",
        "        # list(map(str, box) but then would need to cast on the other end back to float\n",
        "        b = box.astype(int)\n",
        "        detections[image_path].append({\"box\" : b, \"label\" : label, \"score\" : score})\n",
        "\n",
        "        if visualize:\n",
        "            color = label_color(label)\n",
        "\n",
        "            # b = box.astype(int)\n",
        "            draw_box(draw, b, color=color)\n",
        "\n",
        "            caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
        "            draw_caption(draw, b, caption)\n",
        "\n",
        "    if any(score >= min_score for score in scores[0]):\n",
        "        if visualize:\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.axis('off')\n",
        "            plt.imshow(draw)\n",
        "            plt.show()\n",
        "    \n",
        "print(\"Finished, time per image:\", total_time/len(image_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P3nBtjv81_J"
      },
      "source": [
        "Run an evaluation script to get the mean average precision (mAP) of the CNN. \n",
        "\n",
        "mAP is a model evaluation metric that is relative (aka it can be challenging to compare mAP values across datasets), but a great general metric for different models and approaches to detection objects on the same dataset. \n",
        "\n",
        "Read more about mAP here: https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKEvUTb1JC0"
      },
      "source": [
        "#! keras-retinanet/keras_retinanet/bin/evaluate.py csv data/annotations_test.csv data/classes.csv snapshots/test_model.h5\n",
        "\n",
        "precision_metrics = subprocess.check_output(['keras-retinanet/keras_retinanet/bin/evaluate.py', 'csv', testing_data_file, classes_file, model_path]).decode(\"utf-8\")\n",
        "model_summary = str('Model {m} was generated using {e} epochs, {s} steps and {b} batches'.format(m=model_name, e=epoch_final, s=step_number, b=batch_size_number))\n",
        "print(model_summary)\n",
        "print(precision_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SyoUJ6REKvJ"
      },
      "source": [
        "### Export detections##\n",
        "Write out the detections to a json file that can be used in a GIS for  spatial databases and/or visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-zIVPAVEKvH"
      },
      "source": [
        "class MyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(MyEncoder, self).default(obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd3NXa3wEKvK"
      },
      "source": [
        "output_file_name = 'data/new_detections.json'\n",
        "with open(output_file_name, 'w') as fp:\n",
        "    json.dump(detections, fp, cls=MyEncoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6WjaR1EKvM"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/\" + output_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgDzXkfvcOhk"
      },
      "source": [
        "### Export model and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ndsVTJcOhl"
      },
      "source": [
        "# export metrics (fast)\n",
        "files.download(\"/content/output/Epoch-{n}.png\".format(n=epoch_final))\n",
        "files.download(\"/content/output/Epoch-{n}.csv\".format(n=epoch_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1lQjgH6Sy5z"
      },
      "source": [
        "#export inference model (slow)\n",
        "files.download(\"/content/{m}\".format(m=model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Nz2tu_xDqg"
      },
      "source": [
        "#export training model (even slower)\n",
        "files.download(\"/content/{m}\".format(m=best_model_training))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ24IwaycOhl"
      },
      "source": [
        "#### At the end of this script you should have a single `json` file downloaded (in addition model training metrics in `png` and `csv` format), and two versions of the final model: an inferential version for deployment and a training version for additional training applications. Drop the `json` in the Google Drive folder so it can be imported in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ3OzkdicOhm"
      },
      "source": [
        "Next steps:\n",
        "\n",
        "5) export CNN outputs"
      ]
    }
  ]
}