{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_from_multiple_sets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOnYGAHc+Y5TBe9z796K1/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/CNN_tools/blob/main/training_from_multiple_sets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl0scVF_G5VV"
      },
      "source": [
        "# Import tiles from one dataset and run a model from another dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeRHbK8bdwLy"
      },
      "source": [
        "**Before running this script, enter the drive folder links of each dataset you would like to use to train the model. Each drive folder should include (1) tiled images, (2) the `tiling_scheme.json` file, and (3) the training data associated with each tile set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRUtUEXGG5Ve"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/CNN_tools/blob/main/training_from_multiple_sets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "#####  <center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frX2c-fQ8Qkq",
        "outputId": "a4975637-a97b-4e11-beb0-5e466e037aa3"
      },
      "source": [
        "# set variable to the destination google drive folder you want to pull from\n",
        "drive_folders = ['https://drive.google.com/drive/folders/1DKAp-k2cHWFj9rLNhNL4i6dKoPcR3Gn6',\n",
        "                'https://drive.google.com/drive/folders/1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D']\n",
        "\n",
        "# manually assign IDs to the dataset, if wanted, for output labeling\n",
        "dataset_IDs = ['HI2016', 'HI2015']\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os, numpy as np\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "dataset_list = np.empty(len(drive_folders), dtype=object) \n",
        "for num,entry in enumerate(drive_folders):\n",
        "  print(\"new directory: dataset_{c}\".format(c=num))\n",
        "  local_download_path = os.path.expanduser(\"dataset_{c}\".format(c=num))\n",
        "  try:\n",
        "    os.makedirs(local_download_path)\n",
        "  except: pass\n",
        "  pointer = str(\"'\" + entry.split(\"/\")[-1] + \"'\" + \" in parents\")\n",
        "\n",
        "  file_list = drive.ListFile(\n",
        "      {'q': pointer}).GetList()\n",
        "\n",
        "    # 3. Create & download filetypes of interest by id.\n",
        "  count = 0\n",
        "  image_list = []\n",
        "  for f in file_list:\n",
        "    fname = os.path.join(local_download_path, f['title'])\n",
        "    if fname.endswith(\".png\"):\n",
        "      image_list.append(fname)\n",
        "      count += 1\n",
        "      if count % 10 == 0:\n",
        "        print(\"dataset_{e}: {c} tiles pulled\".format(e=num, c=count))\n",
        "      f_ = drive.CreateFile({'id': f['id']})\n",
        "      f_.GetContentFile(fname)\n",
        "    elif fname.endswith(\".csv\") or fname.endswith(\".json\"):\n",
        "      f_ = drive.CreateFile({'id': f['id']})\n",
        "      f_.GetContentFile(fname)\n",
        "  dataset_list[num] = {\"dataset_name\": local_download_path,\n",
        "                  \"annotations_file\": \"annotations_placeholder\",\n",
        "                  \"tiling_scheme_file\": \"tsf_placeholder\",\n",
        "                  \"image_list\": image_list}\n",
        "  print(\"dataset_{e}: {c} tiles pulled\".format(e=num, c=count))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new directory: dataset_0\n",
            "dataset_0: 10 tiles pulled\n",
            "dataset_0: 20 tiles pulled\n",
            "dataset_0: 30 tiles pulled\n",
            "dataset_0: 40 tiles pulled\n",
            "dataset_0: 50 tiles pulled\n",
            "dataset_0: 60 tiles pulled\n",
            "dataset_0: 70 tiles pulled\n",
            "dataset_0: 80 tiles pulled\n",
            "dataset_0: 90 tiles pulled\n",
            "dataset_0: 100 tiles pulled\n",
            "dataset_0: 110 tiles pulled\n",
            "dataset_0: 120 tiles pulled\n",
            "dataset_0: 130 tiles pulled\n",
            "dataset_0: 134 tiles pulled\n",
            "new directory: dataset_1\n",
            "dataset_1: 10 tiles pulled\n",
            "dataset_1: 20 tiles pulled\n",
            "dataset_1: 30 tiles pulled\n",
            "dataset_1: 40 tiles pulled\n",
            "dataset_1: 50 tiles pulled\n",
            "dataset_1: 60 tiles pulled\n",
            "dataset_1: 70 tiles pulled\n",
            "dataset_1: 80 tiles pulled\n",
            "dataset_1: 90 tiles pulled\n",
            "dataset_1: 100 tiles pulled\n",
            "dataset_1: 110 tiles pulled\n",
            "dataset_1: 120 tiles pulled\n",
            "dataset_1: 130 tiles pulled\n",
            "dataset_1: 140 tiles pulled\n",
            "dataset_1: 150 tiles pulled\n",
            "dataset_1: 160 tiles pulled\n",
            "dataset_1: 170 tiles pulled\n",
            "dataset_1: 180 tiles pulled\n",
            "dataset_1: 190 tiles pulled\n",
            "dataset_1: 200 tiles pulled\n",
            "dataset_1: 210 tiles pulled\n",
            "dataset_1: 220 tiles pulled\n",
            "dataset_1: 230 tiles pulled\n",
            "dataset_1: 240 tiles pulled\n",
            "dataset_1: 244 tiles pulled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfrFEWZJTBva"
      },
      "source": [
        "### Identify necessary files from among files in the input directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxK-jdv2RVkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4f4749-4a17-4ac0-fa9a-770a428057a8"
      },
      "source": [
        "import csv, json\n",
        "for num,dataset in enumerate(dataset_list):\n",
        "  for fname in os.listdir(dataset[\"dataset_name\"]):\n",
        "    if fname.endswith(\".csv\"): \n",
        "      annotations_candidate = \"{i}/{f}\".format(i=dataset[\"dataset_name\"], f=fname)\n",
        "      with open(annotations_candidate, \"r\") as f:\n",
        "        if next(csv.reader(f, delimiter=\",\"))[0:3] == ['filename', 'file_size', 'file_attributes']:\n",
        "          dataset_list[num][\"annotations_file\"] = annotations_candidate\n",
        "        else: continue\n",
        "\n",
        "    if fname.endswith(\".json\"):\n",
        "      tiling_scheme_candidate = \"{i}/{f}\".format(i=dataset[\"dataset_name\"], f=fname)\n",
        "      with open(tiling_scheme_candidate) as f:\n",
        "        try:\n",
        "          image_list = list(json.load(f)[\"tile_pointers\"][\"image_locations\"].keys())\n",
        "          dataset_list[num][\"tiling_scheme_file\"] = tiling_scheme_candidate\n",
        "        except: continue\n",
        "\n",
        "  if dataset_list[num][\"annotations_file\"] == \"annotations_placeholder\":\n",
        "    raise Exception(\"VIA annotations file not found\")\n",
        "  elif dataset_list[num][\"tiling_scheme_file\"] == \"TSF_placeholder\":\n",
        "    raise Exception(\"tiling scheme file not found\")\n",
        "\n",
        "  print(\"{d} annotations file identified as {f}\".format(d=dataset[\"dataset_name\"], f = dataset_list[num][\"annotations_file\"]))\n",
        "  print(\"{d} tiling scheme file identified as {f}\".format(d=dataset[\"dataset_name\"], f = dataset_list[num][\"tiling_scheme_file\"]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset_0 annotations file identified as dataset_0/via_SealCNN_TrainingData2016.csv\n",
            "dataset_0 tiling scheme file identified as dataset_0/tiling_scheme.json\n",
            "dataset_1 annotations file identified as dataset_1/via_SealCNN_TrainingData.csv\n",
            "dataset_1 tiling scheme file identified as dataset_1/tiling_scheme.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMTsew-l2VF"
      },
      "source": [
        "### Reformat annotations from VIA to RetinaNet format\n",
        "The following loop pulls each annotation, line-by-line, from the VIA exported CSV, extracts the necessary information, reformats it into the format that RetinaNet requires (https://github.com/fizyr/keras-retinanet#annotations-format), then reassembles a new CSV line-by-line that RetinaNet can receive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DQ2btGJRl2VG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d2a6be-59d2-40da-dffb-1d19bf7a5e9e"
      },
      "source": [
        "# Create blank list for class names\n",
        "class_list = []\n",
        "image_annotations = []\n",
        "\n",
        "# read each line, parse it, convert it, put it all back together\n",
        "# then drop it in the appropriate subset\n",
        "for num,dataset in enumerate(dataset_list):\n",
        "  with open(dataset[\"annotations_file\"], \"r\") as f:\n",
        "      reader = csv.reader(f, delimiter=\",\")\n",
        "      for line in reader: \n",
        "          # output we want:\n",
        "          # format: path/to/image.jpg,x1,y1,x2,y2,class_name\n",
        "          # example: /data/imgs/img_001.jpg,837,346,981,456,cow\n",
        "          filename = line[0]\n",
        "          if filename == 'filename':\n",
        "              # bypassing comments in csv\n",
        "              continue\n",
        "          filename = \"{d}/{f}\".format(d=dataset[\"dataset_name\"], f=filename)\n",
        "          if '{}' in line[5]:\n",
        "              new_row = [filename,\"\",\"\",\"\",\"\",\"\"]\n",
        "              # create a blank entry for empty images\n",
        "          else:  \n",
        "            # pulling from column named \"region_shape_attributes\"\n",
        "            box_entry = json.loads(line[5])\n",
        "            top_left_x, top_left_y, width, height = box_entry[\"x\"], box_entry[\"y\"], box_entry[\"width\"], box_entry[\"height\"]\n",
        "    \n",
        "            if width == 0 or height == 0:\n",
        "                continue\n",
        "                # skip tiny/empty boxes\n",
        "            \n",
        "            # convert from \"top left and width/height\" to \"x and y values at each corner of the box\"\n",
        "            if top_left_x < 0:\n",
        "                top_left_x = 1\n",
        "            if top_left_y < 0:\n",
        "                top_left_y = 1\n",
        "            x1, x2, y1, y2 = top_left_x, top_left_x + width, top_left_y, top_left_y + height \n",
        "            \n",
        "            # pulling from column named \"region_attributes\" to get class names\n",
        "            name = json.loads(line[6])[\"Age Class\"]\n",
        "\n",
        "            # skip unknown class, in this case. Might be useful in other applications though,\n",
        "            # e.g. total object count irrespective of class\n",
        "            if name == \"Unknown\":\n",
        "                continue\n",
        "\n",
        "            # build list of classes as we encounter new names\n",
        "            if name not in class_list:\n",
        "                class_list.append(name)\n",
        "\n",
        "            # create the annotation row\n",
        "            new_row = [filename, x1, y1, x2, y2, name]\n",
        "          \n",
        "          image_annotations.append(new_row)\n",
        "print(\"{n} annotations compiled with {x} classes: {c}\".format(n=len(image_annotations), x=len(class_list), c=class_list))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7168 annotations compiled with 2 classes: ['Adult', 'Pup']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHApm2aSWbuI"
      },
      "source": [
        "output_dir = \"output_directory\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "output_name = \"annotations.csv\"\n",
        "for ID in dataset_IDs:\n",
        "  output_name = \"{i}_{o}\".format(i=ID, o=output_name)\n",
        "full_annotations_name = output_dir + \"/\" + output_name\n",
        "with open(full_annotations_name, 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(image_annotations)\n",
        "\n",
        "detection_classes = []\n",
        "for i in range(0, len(class_list)):\n",
        "    detection_classes.append([class_list[i], i])\n",
        "full_classes_name = output_dir + '/classes.csv'\n",
        "\n",
        "with open(full_classes_name, 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(detection_classes)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-xdfna11L2F"
      },
      "source": [
        "### Install the Convolutional Neural Network that will do the detections. \n",
        "\n",
        "This section sets up the software and pulls code for a CNN model called \"RetinaNet\" which uses the model \"ResNet-50\" as a subcomponent. This section then loads data for an existing ResNet-50 model (pre-trained for object detection) which we will further train for our task.\n",
        "\n",
        "Disregard any errors or prompts to \"restart runtime\" unless the code stops progressing (then email me at gdl10@duke.edu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-VXXOfd-LXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9f361e-19d3-4741-88b3-494df1fecda6"
      },
      "source": [
        "# install the keras package\n",
        "! pip install keras==2.4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/19/9d8f1c86c09d05369da39b03d011cd689edef86c0e6b2777dbcedc49dfc6/Keras-2.4.0-py2.py3-none-any.whl (170kB)\n",
            "\r\u001b[K     |██                              | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51kB 23.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 61kB 20.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81kB 18.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 92kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 122kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 153kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (1.19.5)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (2.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.12.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (2.4.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.28.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (56.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.3.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (4.7.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4) (3.4.1)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jb7YRXlEUUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496622cc-59d3-423a-be60-8d12de8086b6"
      },
      "source": [
        "# copy the files for RetinaNet\n",
        "# note that this build is now deprecated, but we are fine with that\n",
        "# now pulling from a personal clone that outputs error metrics\n",
        "! git clone https://github.com/gl7176/keras-retinanet.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-retinanet'...\n",
            "remote: Enumerating objects: 6236, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 6236 (delta 0), reused 0 (delta 0), pack-reused 6230\u001b[K\n",
            "Receiving objects: 100% (6236/6236), 13.48 MiB | 36.91 MiB/s, done.\n",
            "Resolving deltas: 100% (4218/4218), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuULO44w6yx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c84708ac-9c63-4962-8cba-b77d4cd8cfd4"
      },
      "source": [
        "# change directory and install RetinaNet from the copied code\n",
        "% cd keras-retinanet\n",
        "\n",
        "! pip install ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-retinanet\n",
            "Processing /content/keras-retinanet\n",
            "Collecting keras-resnet==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/76/d4/a35cbd07381139dda4db42c81b88c59254faac026109022727b45b31bcad/keras-resnet-0.2.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (0.29.22)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (4.1.2.30)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from keras-retinanet==1.0.0) (3.38.0)\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.4.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->keras-retinanet==1.0.0) (2.5.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.10.0)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (0.3.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (56.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.28.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (4.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet==0.2.0->keras-retinanet==1.0.0) (3.1.0)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-1.0.0-cp37-cp37m-linux_x86_64.whl size=168714 sha256=84af477a70e1a834bd7baed3014c5052869db2c8056c71d8f67c7d86e3b4b35e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/9f/57/cb0305f6f5a41fc3c11ad67b8cedfbe9127775b563337827ba\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20486 sha256=a39913d25ffd5e363af3a14c2cb0065ee2b173eff52008a7a74b8447ded8c034\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/09/a5/497a30fd9ad9964e98a1254d1e164bcd1b8a5eda36197ecb3c\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "Successfully installed keras-resnet-0.2.0 keras-retinanet-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_IOPbC44SC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcc4684-9893-4d48-9a9f-944565d4d7a4"
      },
      "source": [
        "! python setup.py build_ext --inplace"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "cythoning keras_retinanet/utils/compute_overlap.pyx to keras_retinanet/utils/compute_overlap.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/keras-retinanet/keras_retinanet/utils/compute_overlap.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'keras_retinanet.utils.compute_overlap' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/keras_retinanet\n",
            "creating build/temp.linux-x86_64-3.7/keras_retinanet/utils\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -c keras_retinanet/utils/compute_overlap.c -o build/temp.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kkeras_retinanet/utils/compute_overlap.c:610\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/keras_retinanet\n",
            "creating build/lib.linux-x86_64-3.7/keras_retinanet/utils\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-a56wZI/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.o -o build/lib.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/keras_retinanet/utils/compute_overlap.cpython-37m-x86_64-linux-gnu.so -> keras_retinanet/utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33S3M0SoI1JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078b7125-8e60-45e3-9b19-2be8b86ab84a"
      },
      "source": [
        "% cd ../\n",
        "\n",
        "# get the pre-trained ResNet-50 model\n",
        "! wget -P data \"https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2021-04-28 13:48:59--  https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210428%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210428T134859Z&X-Amz-Expires=300&X-Amz-Signature=cb2b6aef8f6131f79c09c3e3541b87078a6e1844aa9837d3ec83e3315a4e8ee6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-04-28 13:48:59--  https://github-releases.githubusercontent.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210428%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210428T134859Z&X-Amz-Expires=300&X-Amz-Signature=cb2b6aef8f6131f79c09c3e3541b87078a6e1844aa9837d3ec83e3315a4e8ee6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.110.154, 185.199.111.154, 185.199.109.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.110.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152662144 (146M) [application/octet-stream]\n",
            "Saving to: ‘data/resnet50_coco_best_v2.1.0.h5’\n",
            "\n",
            "resnet50_coco_best_ 100%[===================>] 145.59M  97.9MB/s    in 1.5s    \n",
            "\n",
            "2021-04-28 13:49:01 (97.9 MB/s) - ‘data/resnet50_coco_best_v2.1.0.h5’ saved [152662144/152662144]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78RVPbaTiwHB",
        "outputId": "1a6d9151-969c-4683-fe6c-e947e9a4814d"
      },
      "source": [
        "import shutil\n",
        "print(os.getcwd())\n",
        "for dataset in dataset_list:\n",
        "  shutil.move(dataset[\"dataset_name\"], \"{o}/{d}\".format(o=output_dir, d=dataset[\"dataset_name\"]))\n",
        "  #original = r'original path where the directory is currently stored\\directory name'\n",
        "  #target = r'target path where the directory will be moved\\directory name'\n",
        "\n",
        "#shutil.move(original,target)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "n_sRjr1pdJTN",
        "outputId": "f6ce440c-0154-4ffb-f284-d290f60bc4ee"
      },
      "source": [
        "image_count = 0\n",
        "for dataset in dataset_list:\n",
        "  image_count += len(dataset[\"image_list\"])\n",
        "\n",
        "import subprocess, glob\n",
        "\n",
        "epoch_number = 50\n",
        "batch_size_number = 2\n",
        "step_number = int(image_count/batch_size_number)\n",
        "print(str(step_number) + \" steps\")\n",
        "\n",
        "# terminal code for troubleshooting\n",
        "#! keras-retinanet/keras_retinanet/bin/train.py \\\n",
        "#--weights data/resnet50_coco_best_v2.1.0.h5 \\\n",
        "#--epochs 50 --steps 189 --batch-size 2 \\\n",
        "#csv output_directory/HI2015_HI2016_annotations.csv output_directory/classes.csv\n",
        "\n",
        "# this process takes a while to run, be warned!\n",
        "# you can monitor epoch outputs by output files in the \"output\" folder\n",
        "\n",
        "model_run = subprocess.check_output(['keras-retinanet/keras_retinanet/bin/train.py',\n",
        "                 '--weights', 'data/resnet50_coco_best_v2.1.0.h5',\n",
        "                 '--epochs', str(epoch_number),  '--steps', str(step_number), '--batch-size',\n",
        "                 str(batch_size_number), 'csv', full_annotations_name, full_classes_name]).decode(\"utf-8\")\n",
        "print(model_run)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "189 steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-59b7f4c42d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                  \u001b[0;34m'--weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/resnet50_coco_best_v2.1.0.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                  \u001b[0;34m'--epochs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'--steps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--batch-size'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                  str(batch_size_number), 'csv', full_annotations_name, full_classes_name]).decode(\"utf-8\")\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 411\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['keras-retinanet/keras_retinanet/bin/train.py', '--weights', 'data/resnet50_coco_best_v2.1.0.h5', '--epochs', '50', '--steps', '189', '--batch-size', '2', 'csv', 'output_directory/HI2015_HI2016_annotations.csv', 'output_directory/classes.csv']' died with <Signals.SIGKILL: 9>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhz79W8kiTHx"
      },
      "source": [
        "list_of_files = glob.glob('snapshots/resnet*.h5')\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "epoch_final = latest_file[latest_file.index(\"_csv_\")+5:-3]\n",
        "best_model_training = latest_file.replace(\"/content/\", \"\")\n",
        "print(best_model_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apHCQjSo8WC2"
      },
      "source": [
        "This next section converts the model from training mode to inference mode so it can be used to detect our target objects (seals). Until now we've been updating the model based on its performance; now we're fixing the model in a static \"snapshot\" so we can test it out. This conversion process take a little time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YyMralKEKuz"
      },
      "source": [
        "# note that we are naming our model \"best_model_inference\" and locating it in the \"snapshots\" directory. Customize if wanted\n",
        "model_name = \"best_model_inference\"\n",
        "#! keras-retinanet/keras_retinanet/bin/convert_model.py snapshots/resnet50_csv_10.h5 snapshots/best_model_inference.h5\n",
        "subprocess.run([\"keras-retinanet/keras_retinanet/bin/convert_model.py\", best_model_training, \"snapshots/{m}.h5\".format(m=model_name)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgDzXkfvcOhk"
      },
      "source": [
        "### Export model and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW5W8tUmiYvX"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ndsVTJcOhl"
      },
      "source": [
        "# export metrics (fast)\n",
        "files.download(\"/content/output/Epoch-{n}.png\".format(n=epoch_final))\n",
        "files.download(\"/content/output/Epoch-{n}.csv\".format(n=epoch_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1lQjgH6Sy5z"
      },
      "source": [
        "#export inference model (slow)\n",
        "files.download(\"/content/{m}\".format(m=model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Nz2tu_xDqg"
      },
      "source": [
        "#export training model (even slower)\n",
        "files.download(\"/content/{m}\".format(m=best_model_training))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}