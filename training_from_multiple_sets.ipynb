{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_from_multiple_sets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOLsQcyIwXCxHKpjzxymh1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/CNN_tools/blob/main/training_from_multiple_sets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl0scVF_G5VV"
      },
      "source": [
        "# Import tiles from one dataset and run a model from another dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeRHbK8bdwLy"
      },
      "source": [
        "**Before running this script, enter the drive folder links of each dataset you would like to use to train the model. Each drive folder should include (1) tiled images, (2) the `tiling_scheme.json` file, and (3) the training data associated with each tile set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRUtUEXGG5Ve"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/CNN_tools/blob/main/training_from_multiple_sets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "#####  <center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frX2c-fQ8Qkq",
        "outputId": "fbb44638-3c5c-4367-af91-82fa9e64cbce"
      },
      "source": [
        "# set variable to the destination google drive folder you want to pull from\n",
        "drive_folders = ['https://drive.google.com/drive/folders/1DKAp-k2cHWFj9rLNhNL4i6dKoPcR3Gn6',\n",
        "                'https://drive.google.com/drive/folders/1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D']\n",
        "\n",
        "# manually assign IDs to the dataset, if wanted, for output labeling\n",
        "dataset_IDs = ['HI2016', 'HI2015']\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os, numpy as np\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "dataset_list = np.empty(len(drive_folders), dtype=object) \n",
        "for num,entry in enumerate(drive_folders):\n",
        "  print(\"new directory: dataset_{c}\".format(c=num))\n",
        "  local_download_path = os.path.expanduser(\"dataset_{c}\".format(c=num))\n",
        "  try:\n",
        "    os.makedirs(local_download_path)\n",
        "  except: pass\n",
        "  pointer = str(\"'\" + entry.split(\"/\")[-1] + \"'\" + \" in parents\")\n",
        "\n",
        "  file_list = drive.ListFile(\n",
        "      {'q': pointer}).GetList()\n",
        "\n",
        "    # 3. Create & download filetypes of interest by id.\n",
        "  count = 0\n",
        "  image_list = []\n",
        "  for f in file_list:\n",
        "    fname = os.path.join(local_download_path, f['title'])\n",
        "    if fname.endswith(\".png\"):\n",
        "      image_list.append(fname)\n",
        "      count += 1\n",
        "      if count % 10 == 0:\n",
        "        print(\"dataset_{e}: {c} tiles pulled\".format(e=num, c=count))\n",
        "      f_ = drive.CreateFile({'id': f['id']})\n",
        "      f_.GetContentFile(fname)\n",
        "    elif fname.endswith(\".csv\") or fname.endswith(\".json\"):\n",
        "      f_ = drive.CreateFile({'id': f['id']})\n",
        "      f_.GetContentFile(fname)\n",
        "  dataset_list[num] = {\"dataset_name\": local_download_path,\n",
        "                  \"annotations_file\": \"annotations_placeholder\",\n",
        "                  \"tiling_scheme_file\": \"tsf_placeholder\",\n",
        "                  \"image_list\": image_list}\n",
        "  print(\"dataset_{e}: {c} tiles pulled\".format(e=num, c=count))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new directory: dataset_0\n",
            "dataset_0: 10 tiles pulled\n",
            "dataset_0: 20 tiles pulled\n",
            "dataset_0: 30 tiles pulled\n",
            "dataset_0: 40 tiles pulled\n",
            "dataset_0: 50 tiles pulled\n",
            "dataset_0: 60 tiles pulled\n",
            "dataset_0: 70 tiles pulled\n",
            "dataset_0: 80 tiles pulled\n",
            "dataset_0: 90 tiles pulled\n",
            "dataset_0: 100 tiles pulled\n",
            "dataset_0: 110 tiles pulled\n",
            "dataset_0: 120 tiles pulled\n",
            "dataset_0: 130 tiles pulled\n",
            "dataset_0: 134 tiles pulled\n",
            "new directory: dataset_1\n",
            "dataset_1: 10 tiles pulled\n",
            "dataset_1: 20 tiles pulled\n",
            "dataset_1: 30 tiles pulled\n",
            "dataset_1: 40 tiles pulled\n",
            "dataset_1: 50 tiles pulled\n",
            "dataset_1: 60 tiles pulled\n",
            "dataset_1: 70 tiles pulled\n",
            "dataset_1: 80 tiles pulled\n",
            "dataset_1: 90 tiles pulled\n",
            "dataset_1: 100 tiles pulled\n",
            "dataset_1: 110 tiles pulled\n",
            "dataset_1: 120 tiles pulled\n",
            "dataset_1: 130 tiles pulled\n",
            "dataset_1: 140 tiles pulled\n",
            "dataset_1: 150 tiles pulled\n",
            "dataset_1: 160 tiles pulled\n",
            "dataset_1: 170 tiles pulled\n",
            "dataset_1: 180 tiles pulled\n",
            "dataset_1: 190 tiles pulled\n",
            "dataset_1: 200 tiles pulled\n",
            "dataset_1: 210 tiles pulled\n",
            "dataset_1: 220 tiles pulled\n",
            "dataset_1: 230 tiles pulled\n",
            "dataset_1: 240 tiles pulled\n",
            "dataset_1: 244 tiles pulled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfrFEWZJTBva"
      },
      "source": [
        "### Identify necessary files from among files in the input directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxK-jdv2RVkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f6b0ee-632a-4d0e-ce1e-c8c3db1790af"
      },
      "source": [
        "import csv, json\n",
        "for num,dataset in enumerate(dataset_list):\n",
        "  for fname in os.listdir(dataset[\"dataset_name\"]):\n",
        "    if fname.endswith(\".csv\"): \n",
        "      annotations_candidate = \"{i}/{f}\".format(i=dataset[\"dataset_name\"], f=fname)\n",
        "      with open(annotations_candidate, \"r\") as f:\n",
        "        if next(csv.reader(f, delimiter=\",\"))[0:3] == ['filename', 'file_size', 'file_attributes']:\n",
        "          dataset_list[num][\"annotations_file\"] = annotations_candidate\n",
        "        else: continue\n",
        "\n",
        "    if fname.endswith(\".json\"):\n",
        "      tiling_scheme_candidate = \"{i}/{f}\".format(i=dataset[\"dataset_name\"], f=fname)\n",
        "      with open(tiling_scheme_candidate) as f:\n",
        "        try:\n",
        "          image_list = list(json.load(f)[\"tile_pointers\"][\"image_locations\"].keys())\n",
        "          dataset_list[num][\"tiling_scheme_file\"] = tiling_scheme_candidate\n",
        "        except: continue\n",
        "\n",
        "  if dataset_list[num][\"annotations_file\"] == \"annotations_placeholder\":\n",
        "    raise Exception(\"VIA annotations file not found\")\n",
        "  elif dataset_list[num][\"tiling_scheme_file\"] == \"TSF_placeholder\":\n",
        "    raise Exception(\"tiling scheme file not found\")\n",
        "\n",
        "  print(\"{d} annotations file identified as {f}\".format(d=dataset[\"dataset_name\"], f = dataset_list[num][\"annotations_file\"]))\n",
        "  print(\"{d} tiling scheme file identified as {f}\".format(d=dataset[\"dataset_name\"], f = dataset_list[num][\"tiling_scheme_file\"]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset_0 annotations file identified as dataset_0/via_SealCNN_TrainingData2016.csv\n",
            "dataset_0 tiling scheme file identified as dataset_0/tiling_scheme.json\n",
            "dataset_1 annotations file identified as dataset_1/via_SealCNN_TrainingData.csv\n",
            "dataset_1 tiling scheme file identified as dataset_1/tiling_scheme.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eheLKPfBl2VF"
      },
      "source": [
        "### Shuffle and split images into 3 datasets: Training, Testing, Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt_McLbfl2VF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed4ce34-dc48-45a0-f483-1539ea5c72c0"
      },
      "source": [
        "import random\n",
        "# set pseudo-random values for replicability\n",
        "random.seed(3)\n",
        "\n",
        "image_list = []\n",
        "for value in dataset_list:\n",
        "  image_list= image_list + value[\"image_list\"]\n",
        "\n",
        "# shuffle the image list randomly and get total count\n",
        "random.shuffle(image_list)\n",
        "total_count = len(image_list)\n",
        "\n",
        "# set indices for breaking up the total dataset into TTV parts\n",
        "valid_fraction, train_fraction = 0.2, 0.8\n",
        "\n",
        "# spit error if the math don't add up\n",
        "if (sum([valid_fraction, train_fraction]) != 1.0):\n",
        "   raise Exception(\"fractions should add up to 1\")\n",
        "\n",
        "split_index = int(total_count * train_fraction)\n",
        "\n",
        "# use indices to break up dataset into the three parts\n",
        "train_dataset, valid_dataset= image_list[:split_index], image_list[split_index:]\n",
        "print(len(valid_dataset), len(train_dataset))\n",
        "\n",
        "output_dir = \"output_directory\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# spit out CSV listing the image subsets\n",
        "subset_list = []\n",
        "for row in valid_dataset:\n",
        "        subset_list.append([row, \"validation\"])\n",
        "for row in train_dataset:\n",
        "        subset_list.append([row, \"training\"])\n",
        "with open(output_dir + '/subset_list.csv', 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(subset_list)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "76 302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMTsew-l2VF"
      },
      "source": [
        "### Reformat annotations from VIA to RetinaNet format\n",
        "The following loop pulls each annotation, line-by-line, from the VIA exported CSV, extracts the necessary information, reformats it into the format that RetinaNet requires (https://github.com/fizyr/keras-retinanet#annotations-format), then reassembles a new CSV line-by-line that RetinaNet can receive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DQ2btGJRl2VG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d26e2b7-f811-4b17-8f61-d9c20d3a2dbc"
      },
      "source": [
        "# Create blank list for class names\n",
        "class_list = []\n",
        "image_annotations_train, image_annotations_valid = [], []\n",
        "\n",
        "# read each line, parse it, convert it, put it all back together\n",
        "# then drop it in the appropriate subset\n",
        "for num,dataset in enumerate(dataset_list):\n",
        "  with open(dataset[\"annotations_file\"], \"r\") as f:\n",
        "      reader = csv.reader(f, delimiter=\",\")\n",
        "      for line in reader: \n",
        "          # output we want:\n",
        "          # format: path/to/image.jpg,x1,y1,x2,y2,class_name\n",
        "          # example: /data/imgs/img_001.jpg,837,346,981,456,cow\n",
        "          filename = line[0]\n",
        "          if filename == 'filename':\n",
        "              # bypassing comments in csv\n",
        "              continue\n",
        "          filename = \"{d}/{f}\".format(d=dataset[\"dataset_name\"], f=filename)\n",
        "          if '{}' in line[5]:\n",
        "              new_row = [filename,\"\",\"\",\"\",\"\",\"\"]\n",
        "              # create a blank entry for empty images\n",
        "          else:  \n",
        "            # pulling from column named \"region_shape_attributes\"\n",
        "            box_entry = json.loads(line[5])\n",
        "            top_left_x, top_left_y, width, height = box_entry[\"x\"], box_entry[\"y\"], box_entry[\"width\"], box_entry[\"height\"]\n",
        "    \n",
        "            if width == 0 or height == 0:\n",
        "                continue\n",
        "                # skip tiny/empty boxes\n",
        "            \n",
        "            # convert from \"top left and width/height\" to \"x and y values at each corner of the box\"\n",
        "            if top_left_x < 0:\n",
        "                top_left_x = 1\n",
        "            if top_left_y < 0:\n",
        "                top_left_y = 1\n",
        "            x1, x2, y1, y2 = top_left_x, top_left_x + width, top_left_y, top_left_y + height \n",
        "            \n",
        "            # pulling from column named \"region_attributes\" to get class names\n",
        "            name = json.loads(line[6])[\"Age Class\"]\n",
        "\n",
        "            # skip unknown class, in this case. Might be useful in other applications though,\n",
        "            # e.g. total object count irrespective of class\n",
        "            if name == \"Unknown\":\n",
        "                continue\n",
        "\n",
        "            # build list of classes as we encounter new names\n",
        "            if name not in class_list:\n",
        "                class_list.append(name)\n",
        "\n",
        "            # create the annotation row\n",
        "            new_row = [filename, x1, y1, x2, y2, name]\n",
        "\n",
        "            # append the row to the correct subset (training, testing, or validation)\n",
        "            if filename in train_dataset:\n",
        "                image_annotations_train.append(new_row)\n",
        "            else:\n",
        "                image_annotations_valid.append(new_row)\n",
        "\n",
        "tv_ = list(map(len, [image_annotations_train, image_annotations_valid]))\n",
        "tv = list(map(int, [x/sum(tv_)*100 for x in tv_]))\n",
        "print(\"total breakdown of annotations: {n} - {t}% training set, {v}% validation set\".format(t=str(tv[0]), v=str(tv[1]), n=tv_))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total breakdown of annotations: [5630, 1374] - 80% training set, 19% validation set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHApm2aSWbuI"
      },
      "source": [
        "output_name = 'annotations_train.csv'\n",
        "for ID in dataset_IDs:\n",
        "  output_name = \"{i}_{o}\".format(i=ID, o=output_name)\n",
        "training_data_file = \"{d}/{n}\".format(d=output_dir, n=output_name)\n",
        "with open(training_data_file, 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(image_annotations_train)\n",
        "\n",
        "output_name = 'annotations_valid.csv'\n",
        "for ID in dataset_IDs:\n",
        "  output_name = \"{i}_{o}\".format(i=ID, o=output_name)\n",
        "validation_data_file = \"{d}/{n}\".format(d=output_dir, n=output_name)\n",
        "with open(validation_data_file, 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(image_annotations_valid)\n",
        "\n",
        "detection_classes = []\n",
        "for i in range(0, len(class_list)):\n",
        "    detection_classes.append([class_list[i], i])\n",
        "classes_file = \"{d}/{n}\".format(d=output_dir, n=\"classes.csv\")\n",
        "with open(classes_file, 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(detection_classes)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-xdfna11L2F"
      },
      "source": [
        "### Install the Convolutional Neural Network that will do the detections. \n",
        "\n",
        "This section sets up the software and pulls code for a CNN model called \"RetinaNet\" which uses the model \"ResNet-50\" as a subcomponent. This section then loads data for an existing ResNet-50 model (pre-trained for object detection) which we will further train for our task.\n",
        "\n",
        "Disregard any errors or prompts to \"restart runtime\" unless the code stops progressing (then email me at gdl10@duke.edu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-VXXOfd-LXa"
      },
      "source": [
        "# install the keras package\n",
        "! pip install keras==2.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jb7YRXlEUUg"
      },
      "source": [
        "# copy the files for RetinaNet\n",
        "# note that this build is now deprecated, but we are fine with that\n",
        "# now pulling from a personal clone that outputs error metrics\n",
        "! git clone https://github.com/gl7176/keras-retinanet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuULO44w6yx8"
      },
      "source": [
        "# change directory and install RetinaNet from the copied code\n",
        "% cd keras-retinanet\n",
        "\n",
        "! pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_IOPbC44SC"
      },
      "source": [
        "! python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33S3M0SoI1JI"
      },
      "source": [
        "% cd ../\n",
        "\n",
        "# get the pre-trained ResNet-50 model\n",
        "! wget -P data \"https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78RVPbaTiwHB"
      },
      "source": [
        "import shutil\n",
        "print(os.getcwd())\n",
        "for dataset in dataset_list:\n",
        "  shutil.move(dataset[\"dataset_name\"], \"{o}/{d}\".format(o=output_dir, d=dataset[\"dataset_name\"]))\n",
        "  #original = r'original path where the directory is currently stored\\directory name'\n",
        "  #target = r'target path where the directory will be moved\\directory name'\n",
        "\n",
        "#shutil.move(original,target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_sRjr1pdJTN"
      },
      "source": [
        "image_count = 0\n",
        "for dataset in dataset_list:\n",
        "  image_count += len(dataset[\"image_list\"])\n",
        "\n",
        "import subprocess, glob\n",
        "\n",
        "epoch_number = 50\n",
        "batch_size_number = 2\n",
        "step_number = int(image_count/batch_size_number)\n",
        "print(str(step_number) + \" steps\")\n",
        "\n",
        "# terminal code for troubleshooting\n",
        "#! keras-retinanet/keras_retinanet/bin/train.py \\\n",
        "#--weights data/resnet50_coco_best_v2.1.0.h5 \\\n",
        "#--epochs 50 --steps 189 --batch-size 2 \\\n",
        "#csv output_directory/HI2015_HI2016_annotations.csv output_directory/classes.csv\n",
        "\n",
        "# this process takes a while to run, be warned!\n",
        "# you can monitor epoch outputs by output files in the \"output\" folder\n",
        "\n",
        "model_run = subprocess.check_output(['keras-retinanet/keras_retinanet/bin/train.py',\n",
        "                 '--weights', 'data/resnet50_coco_best_v2.1.0.h5',\n",
        "                 '--epochs', str(epoch_number),  '--steps', str(step_number), '--batch-size',\n",
        "                 str(batch_size_number), 'csv', training_data_file, classes_file,\n",
        "                 '--val-annotations', validation_data_file]).decode(\"utf-8\")\n",
        "print(model_run)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhz79W8kiTHx"
      },
      "source": [
        "list_of_files = glob.glob('snapshots/resnet*.h5')\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "epoch_final = latest_file[latest_file.index(\"_csv_\")+5:-3]\n",
        "best_model_training = latest_file.replace(\"/content/\", \"\")\n",
        "print(best_model_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apHCQjSo8WC2"
      },
      "source": [
        "This next section converts the model from training mode to inference mode so it can be used to detect our target objects (seals). Until now we've been updating the model based on its performance; now we're fixing the model in a static \"snapshot\" so we can test it out. This conversion process take a little time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YyMralKEKuz"
      },
      "source": [
        "# note that we are naming our model \"best_model_inference\" and locating it in the \"snapshots\" directory. Customize if wanted\n",
        "model_name = \"best_model_inference\"\n",
        "#! keras-retinanet/keras_retinanet/bin/convert_model.py snapshots/resnet50_csv_10.h5 snapshots/best_model_inference.h5\n",
        "subprocess.run([\"keras-retinanet/keras_retinanet/bin/convert_model.py\", best_model_training, \"snapshots/{m}.h5\".format(m=model_name)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgDzXkfvcOhk"
      },
      "source": [
        "### Export model and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW5W8tUmiYvX"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ndsVTJcOhl"
      },
      "source": [
        "# export metrics (fast)\n",
        "files.download(\"/content/output/Epoch-{n}.png\".format(n=epoch_final))\n",
        "files.download(\"/content/output/Epoch-{n}.csv\".format(n=epoch_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1lQjgH6Sy5z"
      },
      "source": [
        "#export inference model (slow)\n",
        "files.download(\"/content/{m}\".format(m=model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Nz2tu_xDqg"
      },
      "source": [
        "#export training model (even slower)\n",
        "files.download(\"/content/{m}\".format(m=best_model_training))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}